"""
org-mode Memory Connector

–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç org-mode —Ñ–∞–π–ª—ã –≤ mem-agent —Ñ–æ—Ä–º–∞—Ç —á–µ—Ä–µ–∑ pandoc.
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã, —Ç–∞–∫ –∏ —Ü–µ–ª—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏.
"""

import os
import subprocess
from pathlib import Path
from typing import Dict, Any, List, Optional
import tempfile
import re
import hashlib
import json
from datetime import datetime

from memory_connectors.base import BaseMemoryConnector


class OrgModeConnector(BaseMemoryConnector):
    """–ö–æ–Ω–Ω–µ–∫—Ç–æ—Ä –¥–ª—è org-mode —Ñ–∞–π–ª–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º pandoc."""
    
    def __init__(self, output_path: str, **kwargs):
        super().__init__(output_path, **kwargs)
        self.skip_existing = kwargs.get('skip_existing', False)
        self.sync_metadata_file = self.output_path / '.org_sync_metadata.json'
        
    @property
    def connector_name(self) -> str:
        return "org-mode PKM Connector"
    
    @property 
    def supported_formats(self) -> list:
        return ['.org', 'directory']
    
    def extract_data(self, source_path: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ org-mode —Ñ–∞–π–ª–æ–≤ —á–µ—Ä–µ–∑ pandoc —Å –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º."""
        source = Path(source_path)
        
        if not source.exists():
            raise FileNotFoundError(f"–ò—Å—Ç–æ—á–Ω–∏–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω: {source_path}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏
        sync_metadata = self._load_sync_metadata()
        
        org_files = []
        
        if source.is_file():
            if source.suffix == '.org':
                org_files = [source]
            else:
                raise ValueError(f"–§–∞–π–ª –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ .org: {source_path}")
        elif source.is_dir():
            org_files = list(source.rglob('*.org'))
            if not org_files:
                raise ValueError(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ .org —Ñ–∞–π–ª–æ–≤ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {source_path}")
        
        print(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(org_files)} org-—Ñ–∞–π–ª–æ–≤")
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ñ–∞–π–ª—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (—Ç–æ–ª—å–∫–æ –∏–∑–º–µ–Ω–µ–Ω–Ω—ã–µ)
        files_to_process = []
        skipped_count = 0
        
        for org_file in org_files:
            if self._should_process_file(org_file, sync_metadata):
                files_to_process.append(org_file)
            else:
                skipped_count += 1
        
        print(f"üì• –ö –æ–±—Ä–∞–±–æ—Ç–∫–µ: {len(files_to_process)} —Ñ–∞–π–ª–æ–≤, –ø—Ä–æ–ø—É—â–µ–Ω–æ: {skipped_count}")
        
        if not files_to_process:
            print("‚ÑπÔ∏è  –í—Å–µ —Ñ–∞–π–ª—ã –∞–∫—Ç—É–∞–ª—å–Ω—ã, –∏–∑–º–µ–Ω–µ–Ω–∏–π –Ω–µ—Ç")
            return {
                'files': [],
                'source_path': str(source),
                'total_files': 0,
                'skipped_files': skipped_count,
                'sync_metadata': sync_metadata
            }
        
        converted_files = []
        
        for org_file in files_to_process:
            print(f"üìÑ –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É—é: {org_file.name}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ pandoc
            if not self._check_pandoc():
                raise RuntimeError("pandoc –Ω–µ –Ω–∞–π–¥–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ pandoc –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ org-—Ñ–∞–π–ª–æ–≤.")
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ pandoc
            try:
                with tempfile.NamedTemporaryFile(mode='w+', suffix='.md', delete=False) as tmp_file:
                    cmd = [
                        'pandoc',
                        str(org_file),
                        '-f', 'org',
                        '-t', 'markdown',
                        '-o', tmp_file.name
                    ]
                    
                    result = subprocess.run(cmd, capture_output=True, text=True, check=True)
                    
                    # –ß–∏—Ç–∞–µ–º –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π markdown
                    with open(tmp_file.name, 'r', encoding='utf-8') as f:
                        markdown_content = f.read()
                    
                    # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                    os.unlink(tmp_file.name)
                    
                    converted_files.append({
                        'filename': org_file.stem,
                        'original_path': str(org_file),
                        'content': markdown_content,
                        'relative_path': str(org_file.relative_to(source if source.is_dir() else source.parent)),
                        'org_file_obj': org_file  # –î–æ–±–∞–≤–ª—è–µ–º –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
                    })
                    
            except subprocess.CalledProcessError as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ {org_file.name}: {e.stderr}")
                continue
            except Exception as e:
                print(f"‚ö†Ô∏è  –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {org_file.name}: {e}")
                continue
        
        if converted_files:
            print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ: {len(converted_files)} —Ñ–∞–π–ª–æ–≤")
        else:
            print("‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞")
        
        return {
            'files': converted_files,
            'source_path': str(source),
            'total_files': len(converted_files),
            'skipped_files': skipped_count,
            'sync_metadata': sync_metadata
        }
    
    def organize_data(self, extracted_data: Dict[str, Any]) -> Dict[str, Any]:
        """–û—Ä–≥–∞–Ω–∏–∑—É–µ—Ç –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–∞–º—è—Ç–∏."""
        files = extracted_data['files']
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏
        entities = []
        user_info = None
        
        for file_data in files:
            content = file_data['content']
            filename = file_data['filename']
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ñ–∞–π–ª–∞ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
            if self._is_user_profile(content, filename):
                # –≠—Ç–æ –ø—Ä–æ—Ñ–∏–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                user_info = {
                    'filename': filename,
                    'content': content,
                    'type': 'user_profile',
                    'org_file_obj': file_data.get('org_file_obj')  # –î–æ–±–∞–≤–ª—è–µ–º –¥–ª—è —Ç—Ä–µ–∫–∏–Ω–≥–∞
                }
            else:
                # –≠—Ç–æ —Å—É—â–Ω–æ—Å—Ç—å
                entities.append({
                    'filename': filename,
                    'content': content,
                    'type': self._classify_entity_type(content, filename),
                    'original_path': file_data['original_path'],
                    'org_file_obj': file_data.get('org_file_obj')  # –î–æ–±–∞–≤–ª—è–µ–º –¥–ª—è —Ç—Ä–µ–∫–∏–Ω–≥–∞
                })
        
        # –ï—Å–ª–∏ –Ω–µ—Ç —è–≤–Ω–æ–≥–æ –ø—Ä–æ—Ñ–∏–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, —Å–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—ã–π
        if not user_info:
            user_info = self._create_default_user_profile(extracted_data)
        
        return {
            'user_profile': user_info,
            'entities': entities,
            'metadata': {
                'source_path': extracted_data['source_path'],
                'conversion_method': 'pandoc',
                'total_files': len(files),
                'skipped_files': extracted_data.get('skipped_files', 0),
                'sync_metadata': extracted_data.get('sync_metadata', {})
            }
        }
    
    def generate_memory_files(self, organized_data: Dict[str, Any]) -> None:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ñ–∞–π–ª—ã –ø–∞–º—è—Ç–∏ –≤ mem-agent —Ñ–æ—Ä–º–∞—Ç–µ."""
        self.ensure_output_dir()
        
        entities_dir = self.output_path / "entities"
        entities_dir.mkdir(exist_ok=True)
        
        # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏
        sync_metadata = organized_data['metadata'].get('sync_metadata', {})
        updated_files = []
        
        # –°–æ–∑–¥–∞–µ–º user.md —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–æ—Ñ–∏–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        user_profile = organized_data['user_profile']
        if user_profile and user_profile.get('content'):
            user_file = self.output_path / "user.md"
            print(f"üìù –°–æ–∑–¥–∞—é –ø—Ä–æ—Ñ–∏–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {user_file}")
            with open(user_file, 'w', encoding='utf-8') as f:
                f.write(self._enhance_user_profile(user_profile))
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ—Ñ–∏–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Ç–æ–∂–µ
            if 'org_file_obj' in user_profile:
                self._update_file_metadata(user_profile['org_file_obj'], 'user.md', sync_metadata)
                updated_files.append('user')
        
        # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª—ã —Å—É—â–Ω–æ—Å—Ç–µ–π
        entities = organized_data['entities']
        entity_links = []
        
        for entity in entities:
            entity_filename = f"{entity['filename']}.md"
            entity_file = entities_dir / entity_filename
            entity_links.append(f"- [[{entity['filename']}]] - {entity['type']}")
            
            print(f"üìù –°–æ–∑–¥–∞—é —Å—É—â–Ω–æ—Å—Ç—å: {entity_file}")
            with open(entity_file, 'w', encoding='utf-8') as f:
                f.write(self._enhance_entity_content(entity))
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è —ç—Ç–æ–≥–æ —Ñ–∞–π–ª–∞
            if 'org_file_obj' in entity:
                self._update_file_metadata(entity['org_file_obj'], entity_filename, sync_metadata)
                updated_files.append(entity['filename'])
        
        # –û–±–Ω–æ–≤–ª—è–µ–º user.md —Å —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ —Å—É—â–Ω–æ—Å—Ç–∏ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –Ω–æ–≤—ã–µ)
        if entity_links and user_profile:
            user_file = self.output_path / "user.md"
            with open(user_file, 'a', encoding='utf-8') as f:
                f.write("\n\n## –°–≤—è–∑–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ org-mode\n")
                f.write("\n".join(entity_links))
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏
        self._save_sync_metadata(sync_metadata)
        
        total_files = len(entities)
        skipped_files = organized_data['metadata'].get('skipped_files', 0)
        
        if total_files > 0:
            print(f"‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: user.md + {total_files} —Å—É—â–Ω–æ—Å—Ç–µ–π")
            print(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {total_files}, –ø—Ä–æ–ø—É—â–µ–Ω–æ: {skipped_files}")
            if updated_files:
                print(f"üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã: {', '.join(updated_files[:5])}{'...' if len(updated_files) > 5 else ''}")
        else:
            print(f"‚ÑπÔ∏è  –ù–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è (–ø—Ä–æ–ø—É—â–µ–Ω–æ: {skipped_files})")
    
    def _check_pandoc(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ pandoc –≤ —Å–∏—Å—Ç–µ–º–µ."""
        try:
            subprocess.run(['pandoc', '--version'], capture_output=True, check=True)
            return True
        except (subprocess.CalledProcessError, FileNotFoundError):
            return False
    
    def _is_user_profile(self, content: str, filename: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ–∞–π–ª –ø—Ä–æ—Ñ–∏–ª–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."""
        profile_indicators = [
            'about_me', 'profile', '—Ä–µ–∑—é–º–µ', 'cv', 'bio',
            '—Å–∞–º–æ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è', '–∏–Ω—Ç–µ—Ä–≤—å—é', '—Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ'
        ]
        
        filename_lower = filename.lower()
        content_lower = content.lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–º—è —Ñ–∞–π–ª–∞
        for indicator in profile_indicators:
            if indicator in filename_lower:
                return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
        if any(word in content_lower for word in ['–º–µ–Ω—è –∑–æ–≤—É—Ç', 'my name is', '–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –æ–ø—ã—Ç']):
            return True
        
        return False
    
    def _classify_entity_type(self, content: str, filename: str) -> str:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Ç–∏–ø —Å—É—â–Ω–æ—Å—Ç–∏ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É."""
        filename_lower = filename.lower()
        content_lower = content.lower()
        
        if any(word in filename_lower for word in ['nixos', 'linux', 'system']):
            return '–°–∏—Å—Ç–µ–º–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è'
        elif any(word in filename_lower for word in ['notes', '–∑–∞–º–µ—Ç–∫–∏', 'todo']):
            return '–õ–∏—á–Ω—ã–µ –∑–∞–º–µ—Ç–∫–∏ –∏ –∑–∞–¥–∞—á–∏'
        elif any(word in content_lower for word in ['–ø—Ä–æ–µ–∫—Ç', 'project', '—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞']):
            return '–ü—Ä–æ–µ–∫—Ç'
        elif any(word in content_lower for word in ['–∫–æ–º–ø–∞–Ω–∏—è', '—Ä–∞–±–æ—Ç–∞', 'company']):
            return '–û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è'
        else:
            return '–°–ø—Ä–∞–≤–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è'
    
    def _create_default_user_profile(self, extracted_data: Dict[str, Any]) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–µ—Ç –±–∞–∑–æ–≤—ã–π –ø—Ä–æ—Ñ–∏–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."""
        return {
            'filename': 'user_profile',
            'content': f"""# –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å (–∏–∑ org-mode)

## –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
- **–ò—Å—Ç–æ—á–Ω–∏–∫**: org-mode —Ñ–∞–π–ª—ã –∏–∑ {extracted_data['source_path']}
- **–§–∞–π–ª–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ**: {extracted_data['total_files']}
- **–ú–µ—Ç–æ–¥ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏**: pandoc

## Personal Knowledge Management
–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≤–µ–¥–µ—Ç –∑–∞–º–µ—Ç–∫–∏ –≤ org-mode —Ñ–æ—Ä–º–∞—Ç–µ, —á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞:
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Emacs –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–æ—Ä–∞
- –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—é –∑–Ω–∞–Ω–∏—è–º–∏  
- –¢–µ—Ö–Ω–∏—á–µ—Å–∫—É—é –≥—Ä–∞–º–æ—Ç–Ω–æ—Å—Ç—å –∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
""",
            'type': 'generated_profile'
        }
    
    def _enhance_user_profile(self, user_profile: Dict[str, Any]) -> str:
        """–£–ª—É—á—à–∞–µ—Ç –ø—Ä–æ—Ñ–∏–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π."""
        content = user_profile['content']
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ org-mode
        if user_profile['type'] == 'generated_profile':
            return content
        
        # –î–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–æ—Ñ–∏–ª–µ–π –¥–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        enhanced = f"""# –ü—Ä–æ—Ñ–∏–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (org-mode PKM)

*–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –∏–∑ org-mode —á–µ—Ä–µ–∑ pandoc*

{content}

## –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —Å—Ä–µ–¥–∞
- **PKM —Å–∏—Å—Ç–µ–º–∞**: org-mode (Emacs)
- **–§–æ—Ä–º–∞—Ç –∏—Å—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤**: .org
- **–ú–µ—Ç–æ–¥ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏**: pandoc ‚Üí markdown
"""
        return enhanced
    
    def _enhance_entity_content(self, entity: Dict[str, Any]) -> str:
        """–£–ª—É—á—à–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—É—â–Ω–æ—Å—Ç–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏."""
        content = entity['content']
        
        header = f"""# {entity['filename'].replace('_', ' ').title()}

*–¢–∏–ø: {entity['type']}*
*–ò—Å—Ç–æ—á–Ω–∏–∫: {entity['original_path']}*
*–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –∏–∑ org-mode*

---

{content}

---

*–°–≤—è–∑–∞–Ω–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –±—É–¥—É—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Å–∏—Å—Ç–µ–º–æ–π –ø–∞–º—è—Ç–∏*
"""
        return header
    
    def _load_sync_metadata(self) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –∏–∑ —Ñ–∞–π–ª–∞."""
        if not self.sync_metadata_file.exists():
            return {
                'version': '1.0',
                'last_sync': None,
                'files': {}  # file_path -> {'hash': str, 'last_modified': str, 'output_file': str}
            }
        
        try:
            with open(self.sync_metadata_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (json.JSONDecodeError, FileNotFoundError):
            print("‚ö†Ô∏è  –ü–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏, —Å–æ–∑–¥–∞—é –Ω–æ–≤—ã–µ")
            return {
                'version': '1.0',
                'last_sync': None,
                'files': {}
            }
    
    def _save_sync_metadata(self, metadata: Dict[str, Any]) -> None:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –≤ —Ñ–∞–π–ª."""
        metadata['last_sync'] = datetime.now().isoformat()
        self.ensure_output_dir()
        
        with open(self.sync_metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    def _calculate_file_hash(self, file_path: Path) -> str:
        """–í—ã—á–∏—Å–ª—è–µ—Ç SHA256 —Ö–µ—à —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–∞."""
        hasher = hashlib.sha256()
        try:
            with open(file_path, 'rb') as f:
                # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª –±–ª–æ–∫–∞–º–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
                for chunk in iter(lambda: f.read(4096), b""):
                    hasher.update(chunk)
            return hasher.hexdigest()
        except (IOError, OSError) as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {file_path}: {e}")
            return ""
    
    def _should_process_file(self, org_file: Path, sync_metadata: Dict[str, Any]) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Ñ–∞–π–ª."""
        if self.skip_existing:
            # –í —Ä–µ–∂–∏–º–µ skip-existing –ø—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ –≤—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª
            output_file = self.output_path / "entities" / f"{org_file.stem}.md"
            if output_file.exists():
                print(f"‚è≠Ô∏è  –ü—Ä–æ–ø—É—Å–∫–∞—é —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π: {org_file.name}")
                return False
        
        file_path_str = str(org_file)
        current_hash = self._calculate_file_hash(org_file)
        
        if not current_hash:
            return False  # –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        if file_path_str in sync_metadata['files']:
            stored_data = sync_metadata['files'][file_path_str]
            if stored_data.get('hash') == current_hash:
                print(f"‚è≠Ô∏è  –§–∞–π–ª –Ω–µ –∏–∑–º–µ–Ω–∏–ª—Å—è: {org_file.name}")
                return False
        
        return True
    
    def _update_file_metadata(self, org_file: Path, output_filename: str, sync_metadata: Dict[str, Any]) -> None:
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª–∞ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏."""
        file_path_str = str(org_file)
        file_hash = self._calculate_file_hash(org_file)
        
        sync_metadata['files'][file_path_str] = {
            'hash': file_hash,
            'last_modified': datetime.fromtimestamp(org_file.stat().st_mtime).isoformat(),
            'output_file': output_filename,
            'processed_at': datetime.now().isoformat()
        }